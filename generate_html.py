import requests
import os

AIPIPE_TOKEN = os.environ.get("AIPIPE_TOKEN") 
AIPIPE_URL = "https://aipipe.org/openrouter/v1/chat/completions"
MODEL = "qwen/qwen3-coder:free"  # change to other "free" models if needed

def get_code_from_llm(brief, checks, attachments):
    prompt = f"""You are an expert web developer. Generate a beautiful, professional, responsive HTML5 and JS app using Tailwind CSS for the following task.

Requirements:
- Use [Tailwind CSS via CDN](https://cdn.jsdelivr.net/npm/tailwindcss@2.2.19/dist/tailwind.min.css) for all stylingâ€”make the UI modern and visually appealing.
- Use Google Fonts for headings if possible.
- Make it fully responsive and mobile-friendly.
- Add meaningful page structure (centered layout, spacing, shadows, cards, headers, etc).
- All buttons/inputs should be clear, large enough, and easy to interact with.
- Include placeholder icons if visually helpful.

Brief:
{brief}

Checks:
{checks}

Attachments (descriptions only, do not depend on file contents unless sample given):
{attachments}

VERY IMPORTANT:
- Output only a complete, ready-to-serve index.html file with all code (JS in <script> tag, Tailwind via CDN, etc).
- Your result should be visually impressive, feel professional, and have clear separation of concerns in UI.
- No explanation, only code.
- Top-align all content, proper background, padding, rounded, and readable font sizing.
"""


    headers = {
        "Authorization": f"Bearer {AIPIPE_TOKEN}",
        "Content-Type": "application/json",
    }

    payload = {
        "model": MODEL,
        "messages": [
            {"role": "system", "content": "You are a helpful AI code generator."},
            {"role": "user", "content": prompt}
        ],
        "temperature": 0.2,
        "max_tokens": 1400
    }

    resp = requests.post(AIPIPE_URL, headers=headers, json=payload, timeout=60)
    result = resp.json()
    # There may be model variations, but this pattern returns the code as text.
    return result['choices'][0]['message']['content']

# In your generate_task_html:
def generate_task_html(task_json, output_dir="."):
    code = get_code_from_llm(
        task_json.get("brief"), 
        task_json.get("checks"), 
        task_json.get("attachments", [])
    )
    with open(os.path.join(output_dir, "index.html"), "w", encoding="utf-8") as f:
        f.write(code)

def generate_task_readme(task_json, output_dir="."):
    readme_content = f"# {task_json.get('task','Task')}\n\n"
    readme_content += f"**Round:** {task_json.get('round', '')}, **Brief:** {task_json.get('brief', '')}\n\n"
    if task_json.get('attachments'):
        readme_content += "## Attachments\n" + "\n".join(f"- {att['name']}" for att in task_json['attachments']) + "\n"
    if 'checks' in task_json:
        readme_content += "## Checks\n" + "\n".join(f"- {chk}" for chk in task_json['checks']) + "\n"
    readme_content += f"\n**Evaluation URL:** {task_json.get('evaluation_url','')}\n"
    readme_content += "\nThis repo is auto-generated by a Flask-based LLM deployment API. MIT License.\n"
    with open(os.path.join(output_dir, "README.md"), "w", encoding="utf-8") as f:
        f.write(readme_content)

